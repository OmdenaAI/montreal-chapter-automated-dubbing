{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOMbaeXtk81b8ouxDu29EKg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Automated Dubbing App - Final Report\n"],"metadata":{"id":"-8pc-K-mhfNb"}},{"cell_type":"markdown","source":["## Project Description"],"metadata":{"id":"5AoLQxJ4h8n3"}},{"cell_type":"markdown","source":["### Background\n","The challenge aims to develop an AI-powered dubbing app that can automatically dub videos in different languages. The app should be able to detect speech in the original video and generate a synchronized dubbing in the desired language using advanced machine learning and natural language processing techniques. The application will streamline the dubbing process, making it more accessible and cost-effective for various sectors, including education, entertainment, business, tourism, and news."],"metadata":{"id":"RhL8ABK9jnXS"}},{"cell_type":"markdown","source":["### Problem statement\n","Traditional dubbing methods are time-consuming, expensive, and often require a large team of professionals. The AI Dubbing App Challenge aims to provide an innovative solution that can streamline the dubbing process and make it more accessible and affordable."],"metadata":{"id":"kniJX-olkqcl"}},{"cell_type":"markdown","source":["###Stakeholders\n","Content creators, end-users, language learners, entertainment companies, educational institutions, governments, tourism sector, business sector, and individuals with visual impairments."],"metadata":{"id":"ZDyUest_k9FX"}},{"cell_type":"markdown","source":["###Applications\n","The AI-powered dubbing application can cater to diverse domains, such as entertainment, education, documentaries, webinars, conferences, language learning, tourism, business, news, social media, and accessibility for blind people."],"metadata":{"id":"Y1aCoXaBlL-y"}},{"cell_type":"markdown","source":["###Project Goals\n","*  Develop an AI-powered dubbing app that can detect speech in the original video and generate a synchronized dubbing in the desired language.\n","*   Improve the accuracy and quality of the dubbing process using the state-of-the-art machine learning and NLP techniques.\n","Provide an innovative and affordable solution for content creators and language learners.\n","*   Promote inclusivity and accessibility by designing the AI-powered dubbing application to cater to the needs of diverse users, including individuals with visual impairments and non-native speakers.\n","*   Contribute to the advancement of machine learning and NLP techniques by sharing the project's findings, challenges, and successes with the broader scientific and research communities.\n","\n"],"metadata":{"id":"lFKFC8pjlhih"}},{"cell_type":"markdown","source":["## Approach\n","To effectively tackle the problem, the project was broken down into smaller modules described below, with each module focusing on a specific task or functionality. This approach facilitated comprehension, testing, and updates. It also enabled different team members to work on different modules concurrently, thereby enhancing productivity.\n","\n","\n","1.   Video Loading and Audio Extraction Module: Develop a module responsible for loading video files into memory and for extracting the audio track from the video files. Initially, we assume the videos of interest are hosted on YouTube.\n","2.   Speech Recognition Module: Develop a module that integrates a speech recognition system capable of converting spoken words into text.\n","3.   Translation Module: Build a module responsible for translating the recognized text into another language.\n","4.   Audio Generation (Text-to-Speech) Module: Construct a module that converts the transcribed text into synthesized speech. This module should generate a new audio track in the translated language.\n","5.   Dubbing Module: Design a module responsible for combining the translated text with appropriate audio clips or the output from the Text-to-Speech module. This module should handle synchronization and blending of the audio to create the dubbed version.\n","6.   User Interface Module: Develop a user interface module for the app, enabling users to interact with the different modules. Users should be able to select input files, specify languages, and control the dubbing process.\n","\n","For each module we had a task team charged with its implementation. Below we describe the work of each task team in more detail.\n","\n"],"metadata":{"id":"nfygdxzVid0j"}},{"cell_type":"markdown","source":["### Task 1 - video processing\n","This task encompasses the video processing phase of the application. The team decided to use PyTube for this task. The team members report that it has been fruitful journey to learn about PyTube to work on video and audio extraction from a YouTube video. There were many collaborators in this phase but working together helped to figure out the functionality of the library to work towards the application. Learning PyTube was an easy task as there were a number of tutorials and documentation to follow. However, the challenging portion was the recent update of Youtube, which at the end broke some functionality of PyTube. The team was able to come up with a work around by changing a few lines of code in the PyTubeâ€™s cypher.py file as suggested in the PyTube issues on Github."],"metadata":{"id":"N6PrmemKrnhn"}},{"cell_type":"markdown","source":["### Task 2 - speach-to-text\n","The task involves converting speech audio into text format.  The selected approach is based on the WhisperAI pre-trained model. Before making the final selection the team experimented with different speech-to-text models to find the one that best suits the project's requirements. They evaluated the models' performance on numerous languages and speech variations.\n","The chosen WhisperAI model is used to transcribe the input audio file into text. The speech is split into shorter segments (phrases) with associated timestamps.\n","\n"," Speaker gender detection is an important feature that enables better voice recognition and higher quality audio reconstruction from transcribed text. Although speaker gender detection was placed on the backlog list as a \"nice-to-have\" feature, the team made a significant effort to implement it. The proposed method was to use Malaya-Speech library. Unfortunatelly, the team was unable to make this library work, so implementing the speaker gender detection remains a task for a future project."],"metadata":{"id":"kcQBCJfdr9hU"}},{"cell_type":"markdown","source":["###Task 3 - text translation\n","The team decided to use a pretrained model for this task, as training an LLM model from scratch requires a lot of processing power and very large dataset that would be out of reach for the team. They considered several pretrained models, in particular some Hugging Face and PyPi models. They settled on PyPi translate, since this is a multilingual model and one can use one model to translate from one language to another from a large set of languages.\n","\n","The model was easy to implement and worked well. However, the team discovered an issue during the whole pipeline testing. The model's API limits the number of calls, so when processing a large text, one might need to wait 24 hours to finish translation.\n","\n","Another issue the team encountered was that sometimes the translated text is longer than the original and does not fit into the designated time stamps. This is a well known problem documented in several papers. For example\n","https://arxiv.org/pdf/2211.16934v1.pdf\n","and\n","https://arxiv.org/pdf/2212.12137v1.pdf\n","examine this problem and propose rather involved solutions.\n","\n","The team used OpenAI GPT to shorten the translated text and resolve this issue."],"metadata":{"id":"_a-x29wbsI85"}},{"cell_type":"markdown","source":["###Task 4 - text-to-speach\n","The scope of this task was to convert the translated text to speech based on timestamp and voice type (male/female). The team researched and experimented with various libraries and APIs for the task and focused on the approach of converting the text to Mel Spectrogram and then to speech (Vocoder). The team agreed to work on Tacotron model for mel-spectrogram generation and HiFIGAN as vocoder to generate the audio from the mel-spectrogram. However the model was not a multilanguage model and the quality of generated speech was not very good, so a decision to switch to edge-tts API was made. Edge-tts API supports many languages,  it is very fast and, since this is an API, there is no need of storage space to save the models. The generated speech quality is very good for both male and female voices.\n","\n","While concatenating the generated audio segments based on the time stamp the team faced an issue. The audio generated sometimes overlapped because of the difference in speaking rate of the original audio and the generated audio. The team implemented a solution by changing the speed of the speech generated so that the speech generated would fit into the time stamp. That worked well and fit properly in the time stamp. However, in some audio segments, the generated audio was very slow and was not clear so there was some inconsistency. Based on the suggestion by Mohammed Kandil, translation team used LLMs to summarise the text translated to fit in the time stamp, so that text-to-speech team simply generated the speech from the text and concatenated it based on the timestamps."],"metadata":{"id":"HJ_oYLxosNvb"}},{"cell_type":"markdown","source":["###Task 5 - pipeline construction\n","The purview of this task was conatenation of translated audio segments, integration of the new audio with video, the whole pipeline construction and integration of the pipeline with UI.\n","The team had to make sure that each module of the project works separately, and then that all the modules work together when added to the pipeline. The success of this task was instrumental in completion of the whole project."],"metadata":{"id":"PvZ8TNhGsc2-"}},{"cell_type":"markdown","source":["###Task 6 - integration with UI\n","The team started by sketching a UI for the application on Figma. The team members found the experience educational and hope to use Figma in the future. The Figma design was implemented as a StreamLit web application."],"metadata":{"id":"IN-cz6ZWsmvE"}},{"cell_type":"markdown","source":["## Results\n","The projects GitHub repository can be found at\n","\n","https://github.com/OmdenaAI/montreal-chapter-automated-dubbing/tree/main"],"metadata":{"id":"1YxlTlGTijSG"}},{"cell_type":"markdown","source":["## Future Plans\n","The goal of our project was to develop a basic version of the app as a proof of concept for automated dubbing. While working on the project the team came up with a list of more advanced \"nice-to-have\" features such as\n","\n","\n","*   speech diarization\n","*   speaker gender detection\n","*   voice cloning\n","*   lip-sync\n","*   dubbing longer videos (more than 5 min long)\n","*  translating to more languages\n","\n","We are exploring the possibility of initiating another round soon to incorporate these features."],"metadata":{"id":"tEyU4djUkaHN"}},{"cell_type":"markdown","source":["## Conclusion and Social Impact\n","Dubbing videos into different languages allows content creators to reach a wider audience and engage with viewers who may not understand the original language. However, the traditional dubbing process involves human effort, time, and cost, making it a resource-intensive task. By automating this process with recent AI and NLP breakthroughs, our app makes dubbing more accessible and cost-effective. This will have significant implications for various sectors such as education, entertainment, business, tourism, and news, where multilingual content is in demand."],"metadata":{"id":"Ch6FmB6witRf"}},{"cell_type":"code","source":[],"metadata":{"id":"vXwd11rP8BD5"},"execution_count":null,"outputs":[]}]}